"use strict";(globalThis.webpackChunkai_book=globalThis.webpackChunkai_book||[]).push([[899],{3171:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module4-vla/lesson1","title":"Introduction to Vision-Language-Action (VLA) and Speech Recognition with Whisper","description":"Explore the VLA paradigm, and integrate speech recognition using OpenAI\'s Whisper model into ROS 2.","source":"@site/docs/module4-vla/lesson1.mdx","sourceDirName":"module4-vla","slug":"/module4-vla/lesson1","permalink":"/physical-ai-textbook/docs/module4-vla/lesson1","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module4-vla/lesson1.mdx","tags":[],"version":"current","frontMatter":{"title":"Introduction to Vision-Language-Action (VLA) and Speech Recognition with Whisper","description":"Explore the VLA paradigm, and integrate speech recognition using OpenAI\'s Whisper model into ROS 2."},"sidebar":"tutorialSidebar","previous":{"title":"Advanced Perception and AI Integration with Isaac ROS","permalink":"/physical-ai-textbook/docs/module3-isaac/lesson3"},"next":{"title":"LLM-based Task Planning and Reasoning in Robotics","permalink":"/physical-ai-textbook/docs/module4-vla/lesson2"}}');var o=i(4848),s=i(8453);const a={title:"Introduction to Vision-Language-Action (VLA) and Speech Recognition with Whisper",description:"Explore the VLA paradigm, and integrate speech recognition using OpenAI's Whisper model into ROS 2."},r="Introduction to Vision-Language-Action (VLA) and Speech Recognition with Whisper",c={},d=[{value:"The Vision-Language-Action (VLA) Paradigm",id:"the-vision-language-action-vla-paradigm",level:2},{value:"Speech Recognition for Robotics",id:"speech-recognition-for-robotics",level:2},{value:"Introducing OpenAI&#39;s Whisper Model",id:"introducing-openais-whisper-model",level:2},{value:"Integrating Whisper into a ROS 2 System",id:"integrating-whisper-into-a-ros-2-system",level:2},{value:"Conceptual ROS 2 Node for Whisper Integration:",id:"conceptual-ros-2-node-for-whisper-integration",level:3}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"introduction-to-vision-language-action-vla-and-speech-recognition-with-whisper",children:"Introduction to Vision-Language-Action (VLA) and Speech Recognition with Whisper"})}),"\n",(0,o.jsx)(n.p,{children:"Welcome to Module 4: Vision-Language-Action (VLA)! This module brings together all the concepts we've learned so far, culminating in the development of a humanoid robot capable of understanding natural language commands, perceiving its environment, and executing complex actions. We begin by introducing the VLA paradigm and focusing on speech recognition, a critical component for natural human-robot interaction, using OpenAI's Whisper model."}),"\n",(0,o.jsx)(n.h2,{id:"the-vision-language-action-vla-paradigm",children:"The Vision-Language-Action (VLA) Paradigm"}),"\n",(0,o.jsx)(n.p,{children:"The Vision-Language-Action (VLA) paradigm represents a significant leap forward in robotics. It aims to create robots that can:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perceive (Vision)"}),": Understand their surroundings through various sensors (cameras, LiDAR, depth sensors)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Comprehend (Language)"}),": Interpret and respond to human commands given in natural language."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Act (Action)"}),": Execute physical tasks in the real or simulated world based on their perception and understanding."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"VLA systems enable more intuitive and flexible human-robot collaboration, moving beyond pre-programmed behaviors to more adaptable and intelligent robotic systems."}),"\n",(0,o.jsx)(n.h2,{id:"speech-recognition-for-robotics",children:"Speech Recognition for Robotics"}),"\n",(0,o.jsx)(n.p,{children:"For a robot to understand natural language commands, it first needs to convert spoken words into text. This is where Speech Recognition (SR) comes into play. Traditional SR systems often struggle with noise, accents, and varying vocabulary. However, recent advancements in deep learning have led to highly robust models."}),"\n",(0,o.jsx)(n.h2,{id:"introducing-openais-whisper-model",children:"Introducing OpenAI's Whisper Model"}),"\n",(0,o.jsx)(n.p,{children:"OpenAI's Whisper is a general-purpose speech recognition model that demonstrates impressive robustness to accents, background noise, and technical language. It's trained on a large dataset of diverse audio and performs transcription, language identification, and voice activity detection. Its high accuracy makes it an excellent choice for robotics applications where clear and precise command interpretation is crucial."}),"\n",(0,o.jsx)(n.h2,{id:"integrating-whisper-into-a-ros-2-system",children:"Integrating Whisper into a ROS 2 System"}),"\n",(0,o.jsx)(n.p,{children:"To integrate Whisper with a ROS 2 robot, you would typically follow these steps:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Audio Capture"}),": Capture audio from a microphone connected to the robot's computing unit. This audio stream needs to be fed into a ROS 2 topic."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Whisper Processing Node"}),": A ROS 2 node runs the Whisper model. It subscribes to the audio topic, processes the audio using Whisper, and publishes the transcribed text to another ROS 2 topic."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Text Processing"}),": Subsequent ROS 2 nodes can subscribe to this transcribed text topic to interpret commands, extract keywords, or feed it into an LLM for task planning."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"conceptual-ros-2-node-for-whisper-integration",children:"Conceptual ROS 2 Node for Whisper Integration:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# This is a conceptual example. Actual implementation would involve audio device interaction and Whisper API calls.\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData # Example audio message type\n\n# Assume Whisper model interaction is handled by a separate library/API\n# import whisper\n\nclass WhisperROSNode(Node):\n    def __init__(self):\n        super().__init__('whisper_ros_node')\n        self.audio_subscription = self.create_subscription(\n            AudioData,\n            '/audio_in', # Topic where audio data is published\n            self.audio_callback,\n            10\n        )\n        self.text_publisher = self.create_publisher(\n            String,\n            '/transcribed_text', # Topic for publishing transcribed text\n            10\n        )\n        self.get_logger().info('Whisper ROS Node started. Waiting for audio input...')\n        # self.whisper_model = whisper.load_model(\"base\") # Load your Whisper model\n\n    def audio_callback(self, msg: AudioData):\n        self.get_logger().info('Received audio data. Transcribing...')\n        # Convert audio data to a format Whisper can use (e.g., NumPy array)\n        # audio_np = # ... conversion logic ...\n\n        # Perform transcription (conceptual)\n        # result = self.whisper_model.transcribe(audio_np)\n        # transcribed_text = result[\"text\"]\n\n        transcribed_text = \"This is a transcribed example.\" # Placeholder for actual transcription\n\n        if transcribed_text:\n            text_msg = String()\n            text_msg.data = transcribed_text\n            self.text_publisher.publish(text_msg)\n            self.get_logger().info(f'Published transcribed text: \"{transcribed_text}\"')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    whisper_node = WhisperROSNode()\n    rclpy.spin(whisper_node)\n    whisper_node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.p,{children:"By integrating Whisper, our robot gains the ability to listen and understand human commands, setting the stage for more complex interactions and autonomous task execution. In the next lesson, we will explore how Large Language Models can be used to plan robot actions based on these natural language inputs."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var t=i(6540);const o={},s=t.createContext(o);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);