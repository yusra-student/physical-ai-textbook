"use strict";(globalThis.webpackChunkai_book=globalThis.webpackChunkai_book||[]).push([[157],{8453:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>r});var t=o(6540);const i={},s=t.createContext(i);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(s.Provider,{value:n},e.children)}},9068:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module4-vla/lesson2","title":"LLM-based Task Planning and Reasoning in Robotics","description":"Discover how Large Language Models (LLMs) can be used for high-level task planning and symbolic reasoning, converting natural language into robot actions.","source":"@site/docs/module4-vla/lesson2.mdx","sourceDirName":"module4-vla","slug":"/module4-vla/lesson2","permalink":"/physical-ai-textbook/docs/module4-vla/lesson2","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module4-vla/lesson2.mdx","tags":[],"version":"current","frontMatter":{"title":"LLM-based Task Planning and Reasoning in Robotics","description":"Discover how Large Language Models (LLMs) can be used for high-level task planning and symbolic reasoning, converting natural language into robot actions."},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to Vision-Language-Action (VLA) and Speech Recognition with Whisper","permalink":"/physical-ai-textbook/docs/module4-vla/lesson1"},"next":{"title":"Multimodal Perception and Capstone Integration","permalink":"/physical-ai-textbook/docs/module4-vla/lesson3"}}');var i=o(4848),s=o(8453);const a={title:"LLM-based Task Planning and Reasoning in Robotics",description:"Discover how Large Language Models (LLMs) can be used for high-level task planning and symbolic reasoning, converting natural language into robot actions."},r="LLM-based Task Planning and Reasoning in Robotics",l={},c=[{value:"The Challenge of High-Level Planning",id:"the-challenge-of-high-level-planning",level:2},{value:"LLMs for Robotic Task Planning",id:"llms-for-robotic-task-planning",level:2},{value:"Prompt Engineering for Robotic Tasks",id:"prompt-engineering-for-robotic-tasks",level:3},{value:"Example: LLM Prompt and Response",id:"example-llm-prompt-and-response",level:3},{value:"Converting Natural Language into Robot Actions",id:"converting-natural-language-into-robot-actions",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"llm-based-task-planning-and-reasoning-in-robotics",children:"LLM-based Task Planning and Reasoning in Robotics"})}),"\n",(0,i.jsx)(n.p,{children:'Following our exploration of speech recognition with Whisper, this lesson focuses on the "Language" aspect of Vision-Language-Action (VLA): how Large Language Models (LLMs) can be leveraged for high-level task planning and symbolic reasoning in robotics. The goal is to enable robots to understand complex, abstract natural language commands and break them down into a sequence of executable robot actions.'}),"\n",(0,i.jsx)(n.h2,{id:"the-challenge-of-high-level-planning",children:"The Challenge of High-Level Planning"}),"\n",(0,i.jsx)(n.p,{children:"Traditional robotic planning often relies on predefined state machines, symbolic planners, or hardcoded rules. While effective for well-defined tasks, these methods struggle with:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Ambiguity in natural language"}),": Humans don't always give precise, unambiguous commands."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Novel situations"}),": Pre-programmed robots may fail in situations not explicitly accounted for."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Common sense reasoning"}),": Robots often lack the broad understanding of the world that humans possess."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"LLMs, with their vast knowledge base and reasoning capabilities derived from training on massive text datasets, offer a promising solution to these challenges."}),"\n",(0,i.jsx)(n.h2,{id:"llms-for-robotic-task-planning",children:"LLMs for Robotic Task Planning"}),"\n",(0,i.jsx)(n.p,{children:"LLMs can bridge the gap between human language and robot actions by:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Interpreting high-level goals"}),': Understanding abstract commands like "clean the table" or "prepare coffee."']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Decomposing tasks"}),": Breaking down a complex goal into a series of simpler sub-goals or actions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Symbolic reasoning"}),": Inferring implicit information, handling exceptions, and performing common-sense reasoning."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Generating action sequences"}),": Outputting a sequence of robot-executable actions, often represented in a structured format."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"prompt-engineering-for-robotic-tasks",children:"Prompt Engineering for Robotic Tasks"}),"\n",(0,i.jsxs)(n.p,{children:["The key to effectively using LLMs for robotics is ",(0,i.jsx)(n.strong,{children:"prompt engineering"}),". This involves crafting the input query (prompt) to the LLM in a way that guides it to produce the desired robotic action sequence. A good prompt might include:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"System Role"}),': Defining the LLM\'s role (e.g., "You are a robotic task planner...").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task Description"}),": The overall goal the robot needs to achieve."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Available Actions"}),": A list of discrete, predefined actions the robot can perform (e.g., ",(0,i.jsx)(n.code,{children:"pick_up(object)"}),", ",(0,i.jsx)(n.code,{children:"place_down(location)"}),", ",(0,i.jsx)(n.code,{children:"move_to(location)"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Environment Context"}),': Information about the robot\'s current state and the environment (e.g., "The robot is at the kitchen counter. A red mug is on the counter.").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Output Format"}),": Specifying the desired structure for the LLM's response (e.g., JSON, a list of actions)."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"example-llm-prompt-and-response",children:"Example: LLM Prompt and Response"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Prompt (to an LLM via API):"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\"You are a helpful robot assistant. The robot is currently at the charging station.\nGoal: 'Please fetch me a cold drink from the fridge.'\nAvailable Actions:\n- `move_to(location)`: Moves the robot to a specified location.\n- `open(object)`: Opens a specified object (e.g., 'fridge', 'door').\n- `close(object)`: Closes a specified object.\n- `detect_object(object_name)`: Scans for a specific object.\n- `pick_up(object_name)`: Picks up an object if detected.\n- `place_down(object_name, location)`: Places down an object.\n\nGenerate a sequence of actions to achieve the goal in JSON format.\"\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Expected LLM Response (simplified):"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'[\n  {"action": "move_to", "parameters": {"location": "kitchen"}},\n  {"action": "open", "parameters": {"object": "fridge"}},\n  {"action": "detect_object", "parameters": {"object_name": "cold drink"}},\n  {"action": "pick_up", "parameters": {"object_name": "cold drink"}},\n  {"action": "close", "parameters": {"object": "fridge"}},\n  {"action": "move_to", "parameters": {"location": "user"}}\n]\n'})}),"\n",(0,i.jsx)(n.h2,{id:"converting-natural-language-into-robot-actions",children:"Converting Natural Language into Robot Actions"}),"\n",(0,i.jsx)(n.p,{children:"Once the LLM generates a structured action sequence, a ROS 2 node is responsible for parsing this output and translating it into low-level commands that the robot's actuators can execute. This involves:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Parsing"}),": Extracting the action name and its parameters from the LLM's structured output."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Mapping"}),": Mapping the high-level action (e.g., ",(0,i.jsx)(n.code,{children:"move_to"}),") to a corresponding robotic skill or behavior (e.g., calling a navigation service, publishing velocity commands)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Execution"}),": Sending these low-level commands to the robot's motor controllers, navigation stack, or manipulation planners."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"In the next lesson, we will integrate all components: speech recognition, LLM-based planning, and multimodal perception, into a comprehensive capstone project that demonstrates a humanoid robot performing complex tasks based on natural language commands."})]})}function g(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);